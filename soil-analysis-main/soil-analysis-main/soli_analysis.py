# -*- coding: utf-8 -*-
"""Soli_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AwfQ5lDPiSbNcrYaIoDqGxZTYu3i-qaX
"""

!pip install tensorflow numpy pandas matplotlib scikit-learn

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt

# Example: Load images from a directory
datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)

train_data = datagen.flow_from_directory('/content/red_soil.jpg',  # Replace with your image dataset directory
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_data = datagen.flow_from_directory(
    'path_to_images',
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

# Example: Load images from a directory
datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)

# Replace '/content/red_soil_images' with the actual directory
# containing your red soil images and subdirectories for classes (if any).
train_data = datagen.flow_from_directory(
    '/content/red_soil.jpg',
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical', # Choose 'binary' if you have only two classes
    subset='training'
)

# Replace 'path_to_images' with the actual directory
# containing your validation images and subdirectories for classes (if any).
# It's recommended to use a separate directory for validation data.
val_data = datagen.flow_from_directory(
    'path_to_validation_images',
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical', # Choose 'binary' if you have only two classes
    subset='validation'
)

from google.colab import files
import pandas as pd

# Upload your Excel file
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# Load the Excel dataset
data = pd.read_excel(filename)
data.head()



import os
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Define image dimensions
IMG_HEIGHT = 150
IMG_WIDTH = 150

# Assuming your images are in a folder named 'soil_images'
# in the same directory as your notebook, adjust the 'image_folder'
# variable below accordingly if needed
image_folder = 'soil_images'

# Load images and preprocess
images = []
for index, path in enumerate(data["Image_Path"]):
    # Construct the full path to the image
    full_path = os.path.join(image_folder, path)

    # Check if the file exists before loading
    if os.path.exists(full_path):
        img = load_img(full_path, target_size=(IMG_HEIGHT, IMG_WIDTH))
        img_array = img_to_array(img) / 255.0
        images.append(img_array)
    else:
        print(f"Image file not found: {full_path} at index: {index}, path: {path}") # Added index and path for debugging

images = np.array(images)

# ... (rest of your code)

# Encode Soil_Type and Crop_Suitability
label_encoder_soil = LabelEncoder()
data["Soil_Type_Encoded"] = label_encoder_soil.fit_transform(data["Soil_Type"])

label_encoder_crop = LabelEncoder()
data["Crop_Suitability_Encoded"] = label_encoder_crop.fit_transform(data["Crop_Suitability"])

# Prepare input features (nutrients)
nutrients = data[["Nutrient_Nitrogen", "Nutrient_Phosphorus", "Nutrient_Potassium"]].values
scaler = StandardScaler()
nutrients_scaled = scaler.fit_transform(nutrients)

# Combine images and nutrients
X_features = np.hstack((images.reshape(len(images), -1), nutrients_scaled))

# Define output targets
y_soil = data["Soil_Type_Encoded"].values
y_crop = data["Crop_Suitability_Encoded"].values

# Split data into training and testing sets
X_train, X_test, y_soil_train, y_soil_test, y_crop_train, y_crop_test = train_test_split(
    X_features, y_soil, y_crop, test_size=0.2, random_state=42
)

from google.colab import files
import pandas as pd

# Upload your Excel file
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# Load the Excel dataset
data = pd.read_excel(filename)
data.head()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV

# Step 1: Load Excel Data
uploaded_file = "synthetic_soil_data.xlsx"
soil_data = pd.read_excel(uploaded_file)

# Step 2: Generate Synthetic Image Data
num_samples = len(soil_data)
image_data = np.random.rand(num_samples, 64, 64, 3)
image_flattened = image_data.reshape(num_samples, -1)

# Step 3: Preprocess Nutrient Data
nutrient_data = soil_data.iloc[:, :-1].values
scaler = StandardScaler()
nutrients_scaled = scaler.fit_transform(nutrient_data)

# Step 4: Encode Target Labels
label_encoder = LabelEncoder()
target = label_encoder.fit_transform(soil_data['Suitable Crop'])

# Step 5: Combine Features (Nutrients + Images)
# Reduce dimensionality of image features
pca = PCA(n_components=50, random_state=42)
image_reduced = pca.fit_transform(image_flattened)
X_features = np.hstack((image_reduced, nutrients_scaled))

# Balance the dataset
smote = SMOTE(random_state=42)
X_features_balanced, target_balanced = smote.fit_resample(X_features, target)

# Step 6: Split Data
X_train, X_test, y_train, y_test = train_test_split(X_features_balanced, target_balanced, test_size=0.2, random_state=42)

# Step 7: Train Model
# Use Grid Search for Hyperparameter Tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, y_train)
model = grid_search.best_estimator_

# Step 8: Evaluate Model
y_pred = model.predict(X_test)
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# Step 9: Prediction Example
sample_input = X_test[0].reshape(1, -1)
predicted_label = label_encoder.inverse_transform(model.predict(sample_input))
print("Predicted Suitable Crop:", predicted_label[0])